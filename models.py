# -*- coding: utf-8 -*-
"""models

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1toxUl11WRREI-3wTICOjiPGR8pVMl7dN
"""

from __future__ import division
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import numpy as np
import argparse
import random
import cv2
import os
import os.path as osp
import numpy as np

def parse_cfg(cfgfile):
  file=open(cfgfile,"r")
  lines=file.read().split("\n")
  lines=[x for x in lines if len(x)>0]
  lines=[x for x in lines if x[0]!="#"]
  lines=[x.lstrip().rstrip() for x in lines]
  blocks=[]
  for line in lines:
    if line[0]=="[":
      block={}
      block["type"]=line[1:-1].rstrip()
      blocks.append(block)
    else:
      key,value=line.split("=")
      block[key.rstrip()]=value.lstrip()
   
  return blocks

def creat_modules(blocks):
  net_info=blocks[0]
  prev_filters=3
  module_list=nn.ModuleList()
  output_filters=[]
  for index, x in enumerate(blocks[1:]):
    module=nn.Sequential()
    if x["type"]=="convolutional":
      stride=int(x["stride"])
      padding=int(x["pad"])
      filters=int(x["filters"])
      kernel_size=int(x["size"])
      activation=x["activation"]
      try:
        batch_normalize=int(x["batchnormalize"])
        bias=False
      except:
        batch_normalize=0
        bias=True
      if padding:
        pad=(kernel_size -1)//2
      else:
        pad=0
      conv=nn.Conv2d(prev_filters,filters,kernel_size,stride,pad,bias=bias)
      module.add_module("conv_{0}".format(index), conv)
      if batch_normalize:
        bn=nn.BatchNorm2d(filters)
        module.add_module("bn_{0}".format(index),bn)
      if activation=="leaky":
        relu=nn.LeakyReLU(.1,inplace=True)
        module.add_module("relu_{0}".format(index), relu)
    elif x["type"]=="upsample":
      upsample=nn.Upsample(scale_factor=2,mode="bilinear")
      module.add_module("upsample_{0}".format(index),upsample)
    elif x["type"]=="route":
      layers=(x["layers"].split(","))
      start=int(layers[0])
      try:
        end=int(layers[1])
      except:
        end=0
      if start>0:
        start=start-index
      if end>0:
        end=end-index
      route=Empty_layer()
      module.add_module("route_{0}".format(index),route)
      if end<0:
        filters=output_filters[index+start]+output_filters[index+end]
      else:
        filters=output_filters[index+start]
    elif x["type"]=="shortcut":
      shortcut=Empty_layer()
      module.add_module("shortcut_{0}".format(index),shortcut)    

    elif x["type"]=="yolo":
      mask=x["mask"].split(",")
      mask=[int(x) for x in mask]
      anchors=x["anchors"].split(",")
      anchors=[int(a) for a in anchors]
      anchors=[(anchors[i],anchors[i+1]) for i in range(0,len(anchors),2)]
      anchors=[anchors[i] for i in mask]
      yolo=YOLO(anchors)
      module.add_module("yolo_{}".format(index),yolo) 
    

    


    module_list.append(module)
    prev_filters=filters
    output_filters.append(filters)
  return (net_info,module_list)

class Empty_layer(nn.Module):
  def __init__(self):
    super(Empty_layer,self).__init__()

class Darknet(nn.Module):
  def __init__(self,cfg):
   super(Darknet,self).__init__()
   self.blocks=parse_cfg(cfg)
   self.net,self.module_list=creat_modules(self.blocks)
  def forward(self,x,cuda):
    modules=self.blocks[1:]
    outputs={}
    write=0
    for i, module in enumerate(modules):
      module_type=module["type"]
      if module_type=="convolutional" or module_type=="upsample":
        x=self.module_list[i](x)
        
      elif module_type=="route":
        layers=module["layers"]
        layers=layers.split(",")
        layers=[x.lstrip() for x in layers]
        
        layers_2=[]
        for x in layers:
          if "-" in x:
            x=int(x.replace("-",""))*-1
            
            
            
            
            layers_2.append(x)
          else:
            layers_2.append(int(x))
            
        
        layers=layers_2
        print(layers)
        
        if layers[0]>0:
          layers[0]=layers[0]-i
        if len(layers)==1:
          x=outputs[i+layers[0]]
        else:

        
         
          if (layers[1]) > 0:


            layers[1] = layers[1] - i

          
          map1=outputs[i+layers[0]]
          map2=outputs[i+layers[1]]
          x=torch.cat((map1,map2),1)
      elif module_type=="shortcut":
        from_=int(module["from"])
        x=outputs[i-1]+outputs[i+from_]
      elif module_type=="yolo":
        if is_training:
          loss=module_list[i](x,targets)
        else:
          x=module_list[i](x)
       
        if write==0:
          detections=x
          write=1
        else:
          detections=torch.cat((x,detections),1)
      outputs[i]=x    
    return detections
  def load_weights(self,file):
    fp=open(file,"rb")
    header=np.fromfile(fp,dtype=np.int32,count=5)
    self.header=torch.from_numpy(header)
    self.seen=self.header[3]
    weights=np.fromfile(fp,dtype=np.float32)
    print(type(weights))
    for i in range(len(self.module_list)):
      model_type=self.blocks[i+1]["type"]
      ptr=0
      if model_type=="convolutional":
        module=self.module_list[i]
        conv=module[0]
        try:
          batch_normalization=int(self.block[i+1]["batch_normalization"])
        except:
          batch_normalization=0
        if batch_normalization:
          bn=module[1]
          num_bias=bn.bias.numel()
          biases=torch.from_numpy(weights[ptr:ptr+num_bias])
          ptr+=num_bias
          num_weights=bn.weights.numel()
          conv_weights=torch.from_numpy(weights[ptr:ptr+num_weights])
          ptr+=num_weights
          running_mean=torch.from_numpy(weights[ptr:ptr+num_weights])
          ptr+=num_weights
          running_var=torch.from_numpy(weights[ptr:ptr+num_weights])
          biases=biases.view_as(bn.bias.data)
          bn_weights=bn_weights.view_as(bn.weight.data)
          running_mean=running_mean(bn.running_mean.data)
          running_var=running_var(bn.running_var.data)
          bn.bias.data.copy_(biases)
          bn.weight.data.copy_(bn_weights)
          bn.running_mean.data.copy_(running_mean)
          bn.running_var.data.copy_(running_var)
        else:
          num_bias=conv.bias.numel()
          bias=torch.from_numpy(weights[ptr:ptr+num_bias])
          ptr+=num_bias
          bias=bias.view_as(conv.bias.data)
          conv.bias.data.copy_(bias)
      num_weights=conv.weight.numel()
      conv_weights=torch.from_numpy(weights[ptr:ptr+num_weights])
      ptr+=num_weights
      conv_weights=conv_weights.view_as(conv.weight.data)
      conv.weight.data.copy_(conv_weights)
  def write_weights(self,path):
    file=open(path,"wb")
    self.header[3]=self.seen
    self.header.tofile(file)
    for module,module_name in zip(module_list,blocks[1:]):
      if module_name["type"]=="convolution":
        conv=module[0]
        if module_name["batch_normalize"]:
          bn=module[1]
          bn.bias.data.numpy().tofile(file)
          bn.running_mean.data.numpy().tofile(file)
          bn.running_var.data.numpy().tofile(file)
          bn.weights.data.numpy().tofile(file)
        else:
          conv.bias.data.numpy().tofile(file)
        conv.weights.data.numpy().tofile(file)
    file.close()

class YOLO(nn.Module):
  def __init__(self,n_batch,n_classes,anchors,grid,input_dim):
    super(YOLO,self).__init__()
    self.mse_loss=nn.MSELoss()
    self.bce_loss=nn.BCELoss()
    self.ce_loss=nn.CrossEntropyLoss()
    self.nb=n_batch
    self.na=len(anchors)
    self.grid=grid
    self.stride=input_dim/grid
    self.input_dim=input_dim
    self.n_classes=n_classes
  def forward(self,x,targets=None):
    is_training= targets is not None
    nb=self.nb
    na=self.na
    grid=self.grid
    boxattr=x.shape[1]/na
    FloatTensor=torch.cuda.FloatTensor()
    LongTensor=torch.cuda.LongTensor()
    if is_training:


      if x.is_cuda:
        
        self.mse_loss=self.mse_loss.cuda(size_average=True)
        self.bce_loss=self.bce_loss.cuda(size_average=True)
        self.ce_loss=self.ce_loss.cuda()
      prediction=x.view(nb,na,boxattr,grid,grid).permute(0,1,3,4,2)
      x_p=torch.sigmoid(prediction[:,:,:,:,0])
      y_p=torch.sigmoid(prediction[:,:,:,:,1])
      grid_x=torch.arange(grid).repeat(grid,1).view(1,1,grid,grid)
      grid_y=torch.arange(grid).repeat(grid,1).t().view(1,1,grid,grid)
      x_p+=grid_x
      y_p+=grid_y
      w=prediction[:,:,:,:,2]
      h=prediction[:,:,:,:,3]
      anchors=torch.FloatTensor([(int(i[0])/self.stride,int(i[1])/self.stride) for i in anchors])
      anc_w=anchor[:,0].view(1,nb,1,1).unsqueeze(-1)
      anc_h=anchor[:,1].view(1,nb,1,1).unsqueeze(-1)
      obj=torch.sigmoid(prediction[:,:,:,4])
      classes=torch.sigmoid(prediction[:,:,:,5:])
      conf_true=mask
      conf_false=conf_mask-mask
      
      x_t,y_t,wt,ht,obj_t,classes_t,mask=extract_target(anchors,self.grid,target)
      x_t=Variable(x_t.type(FloatTensor),requires_grad=False)
      y_t=Variable(y_t.type(FloatTensor),requires_grad=False)
      w_t=Variable(w_t.type(FloatTensor),requires_grad=False)
      h_t=Variable(h_t.type(FloatTensor),requires_grad=False)
      obj_t=Variable(obj_t.type(FloatTensor),requires_grad=False)
      classes_t=Variable(classes_t.type(LongTensor),requires_grad=False)

      x_loss=mse_loss(x_p[mask],xt[mask])
      y_loss=mse_loss(y_p[mask],yt[mask])
      h_loss=mse_loss(h[mask],ht[mask])
      w_loss=mse_loss(h[mask],wt[mask])
      cls_loss=(1/nb)*(ce_loss(classes[mask],torch.argmax(classes_t[mask],1)))
      conf_loss=bce_loss(obj_t[conf_true],obj[conf_true])+bce_loss(conf[conf_false],obj(conf_false))
      loss=x_loss+y_loss+h_loss+w_loss+cls_loss+conf_loss
      return loss
    else:     
      x=predict_transform(x,anchors,self.input_dim,num_classes,torch.cuda.is_available())

